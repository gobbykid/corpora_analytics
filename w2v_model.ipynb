{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2vec model for GOBBYKID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to display values representing words similarities, we first need to create some models to train with respect to our corpora. Such models will be ablo to use <i>word embeddings</i> in order to retrieve similarities between the words of each corpus. This will be accomplished thanks to the mapping of such words into of feature-vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install required libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to be able to define a w2v (Word2Vec) model, we first need to install some libraries.</br>\n",
    "In particular, we will need to install [Gensim](https://radimrehurek.com/gensim/index.html), [Pandas](https://pandas.pydata.org), libraries thanks to the pip command."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the latest version of gensim:\n",
    "\n",
    "```bash\n",
    "    pip install --upgrade gensim\n",
    "```\n",
    "\n",
    "Or, if you have instead downloaded and unzipped the [source tar.gz]\n",
    "package:\n",
    "\n",
    "```bash\n",
    "    python setup.py install\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once done that, you can import the libraries, as well as the functions defined in the [w2v functions file](w2v_functions.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import pandas as pd\n",
    "from w2v_functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading and Exploring the Dataset\n",
    "We decide to train our model on the basis of our corpora."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first operation is concerned with reading all texts and processing them as sentences.</br>\n",
    "In order to do that we first need to create the two corpora for which we want to build the model, then we will store all the urls of the texts contained inside them into 2 different lists to pass in input to a function developed to store all the tokens of a corpus inside a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_directory = \"Raw/F/\"\n",
    "m_directory = \"Raw/M/\"\n",
    "f_corpus = create_corpus(f_directory)\n",
    "m_corpus = create_corpus(m_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URLS of female authors texts: ['Raw/F/1857_grannys_wonderful_chair.txt', 'Raw/F/1857_the_rambles_of_a_rat.txt', 'Raw/F/1869_little_women.txt', 'Raw/F/1869_mrs_overtheways_remembrances.txt', 'Raw/F/1872_a_dog_of_flanders.txt', 'Raw/F/1877_black_beauty.txt', 'Raw/F/1877_the_cuckoo_clock.txt', 'Raw/F/1886_little_lord_fauntleroy.txt', 'Raw/F/1899_the_story_of_the_treasure_seekers.txt', 'Raw/F/1902_the_tale_of_peter_rabbit.txt', 'Raw/F/1903_rebecca_of_sunnybrook_farm.txt', 'Raw/F/1908_anne_of_green_gables.txt', 'Raw/F/1911_the_secret_garden.txt'] \n",
      "\n",
      "URLS of male authors texts: ['Raw/M/1857_tom_browns_school_days.txt', 'Raw/M/1865_alices_adventures_in_wonderland.txt', 'Raw/M/1869_david_copperfield.txt', 'Raw/M/1871_at_the_back_of_the_north_wind.txt', 'Raw/M/1876_the_adventures_of_tom_sawyer.txt', 'Raw/M/1883_treasure_island.txt', 'Raw/M/1888_the_happy_prince_and_other_tales.txt', 'Raw/M/1894_the_jungle_book.txt'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "f_authors_texts = list()\n",
    "f_titles = list()\n",
    "for url in f_corpus.fileids():\n",
    "    if url != '.DS_Store':\n",
    "        f_authors_texts.append(f_directory+url)\n",
    "\n",
    "m_authors_texts = list()\n",
    "m_titles = list()\n",
    "for url in m_corpus.fileids():\n",
    "    if url != '.DS_Store':\n",
    "        m_authors_texts.append(m_directory+url)\n",
    "\n",
    "print(\"URLS of female authors texts:\", f_authors_texts, '\\n')\n",
    "print(\"URLS of male authors texts:\", m_authors_texts, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_tokens = list_builder(f_authors_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_tokens = list_builder(m_authors_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have the two lists, we need to initialize 2 different models thanks to the Gensim library.</br>\n",
    "Each model will be used for the respective corpus on which it has been trained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">In order to use the \"workers\" parameter, you need first to install cython</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_model = gensim.models.Word2Vec(\n",
    "    window=6,\n",
    "    min_count=2,\n",
    "    workers=8, #This parameter is intended for CPU cores, if you have a CPU with less than 8 cores please modify the value\n",
    "    sg=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_model = gensim.models.Word2Vec(\n",
    "    window=6,\n",
    "    min_count=2,\n",
    "    workers=8, #This parameter is intended for CPU cores, if you have a CPU with less than 8 cores please modify the value\n",
    "    sg=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to build the vocabularies of single tokens for each one of our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_model.build_vocab(f_tokens, progress_per=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_model.build_vocab(m_tokens, progress_per=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43108"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_model.corpus_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step consists on training the two models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2254977, 2632805)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_model.train(f_tokens, total_examples=f_model.corpus_count, epochs=f_model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2218105, 2600725)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_model.train(m_tokens, total_examples=m_model.corpus_count, epochs=m_model.epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will compute some random words as examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('child', 0.7626883387565613),\n",
       " ('creature', 0.7604007720947266),\n",
       " ('accomplished', 0.7479668855667114),\n",
       " ('passenger', 0.7471694946289062),\n",
       " ('soul', 0.7455093860626221),\n",
       " ('alois', 0.7447825074195862),\n",
       " (\"'my\", 0.7436867356300354),\n",
       " ('chap', 0.741326093673706),\n",
       " ('stupid', 0.7399416565895081),\n",
       " ('goose', 0.739003598690033)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_model.wv.most_similar(\"girl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('passenger', 0.7342532277107239),\n",
       " ('delighted', 0.7327626943588257),\n",
       " ('accomplished', 0.7324975728988647),\n",
       " ('doll', 0.7275893092155457),\n",
       " ('dearest', 0.7269617915153503),\n",
       " ('cordelia', 0.7266648411750793),\n",
       " ('child', 0.7213857769966125),\n",
       " (\"'my\", 0.7212626338005066),\n",
       " ('lavander', 0.7201829552650452),\n",
       " ('alois', 0.7198037505149841)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "f_model.wv.most_similar(\"girl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('swallow', 0.8773272633552551),\n",
       " ('prince', 0.8648238778114319),\n",
       " ('emily', 0.8594668507575989),\n",
       " ('creature', 0.8564527034759521),\n",
       " ('minnie', 0.8562754988670349),\n",
       " ('angel', 0.8558725118637085),\n",
       " ('child', 0.8552056550979614),\n",
       " ('blossom', 0.8499689102172852),\n",
       " ('hans', 0.8358373045921326),\n",
       " ('tender', 0.8356804251670837)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_model.wv.most_similar(\"girl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('prince', 0.8320460915565491),\n",
       " ('minnie', 0.8278048634529114),\n",
       " ('angel', 0.8260985016822815),\n",
       " ('blossom', 0.819741427898407),\n",
       " ('murmured', 0.8171412944793701),\n",
       " ('emily', 0.8107228875160217),\n",
       " ('swallow', 0.8087517023086548),\n",
       " ('lover', 0.807963490486145),\n",
       " ('fisherman', 0.8067976236343384),\n",
       " ('child', 0.8024742603302002)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "m_model.wv.most_similar(\"girl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
